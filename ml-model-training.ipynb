{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/safwanahmadsaffi/Shell.ai-Hackathon-2025/blob/main/ml-model-training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "y3cVQwV8STa9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "import numpy as np\n",
        "\n",
        "# Feature engineering function\n",
        "def add_extra_features(df):\n",
        "    # identify numeric features excluding ID and target columns\n",
        "    features = [col for col in df.columns if col not in ['ID'] and not col.startswith('BlendProperty')]\n",
        "    df['feature_sum'] = df[features].sum(axis=1)\n",
        "    df['feature_mean'] = df[features].mean(axis=1)\n",
        "    df['feature_std'] = df[features].std(axis=1)\n",
        "    return df\n",
        "\n",
        "# Load the datasets\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df = pd.read_csv(\"/content/test.csv\")\n",
        "\n",
        "print(f\"Train data shape: {train_df.shape}\")\n",
        "print(f\"Test data shape: {test_df.shape}\")\n",
        "print(f\"Train columns: {list(train_df.columns)}\")\n",
        "print(f\"Test columns: {list(test_df.columns)}\")\n",
        "\n",
        "# Separate features (X) and target variables (y)\n",
        "X_train = train_df.drop([f\"BlendProperty{i}\" for i in range(1, 11)], axis=1)\n",
        "y_train = train_df[[f\"BlendProperty{i}\" for i in range(1, 11)]]\n",
        "\n",
        "# For the test set, drop the 'ID' column\n",
        "X_test = test_df.drop(\"ID\", axis=1)\n",
        "\n",
        "# Add extra statistical features to improve model\n",
        "X_train = add_extra_features(X_train)\n",
        "X_test = add_extra_features(X_test)\n",
        "\n",
        "# Initialize models for ensemble\n",
        "gb_model = GradientBoostingRegressor(n_estimators=200, learning_rate=0.05, max_depth=4, random_state=42)\n",
        "knn_model = KNeighborsRegressor(n_neighbors=7)\n",
        "\n",
        "# Prepare container for predictions\n",
        "predictions = np.zeros((X_test.shape[0], y_train.shape[1]))\n",
        "\n",
        "# Cross-validation setup\n",
        "gkf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "for i, col in enumerate(y_train.columns):\n",
        "    print(f\"Training and validating model for {col}\")\n",
        "    cv_scores = []\n",
        "    for train_idx, val_idx in gkf.split(X_train):\n",
        "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "        y_tr, y_val = y_train[col].iloc[train_idx], y_train[col].iloc[val_idx]\n",
        "        gb_model.fit(X_tr, y_tr)\n",
        "        preds = gb_model.predict(X_val)\n",
        "        cv_scores.append(mean_absolute_percentage_error(y_val, preds))\n",
        "    print(f\"{col} CV MAPE: {np.mean(cv_scores):.5f}\")\n",
        "    # Train on full data\n",
        "    gb_model.fit(X_train, y_train[col])\n",
        "    knn_model.fit(X_train, y_train[col])\n",
        "    preds_gb = gb_model.predict(X_test)\n",
        "    preds_knn = knn_model.predict(X_test)\n",
        "    predictions[:, i] = (preds_gb + preds_knn) / 2\n",
        "\n",
        "# Create and save submission\n",
        "submission_df = pd.DataFrame(predictions, columns=[f\"BlendProperty{i}\" for i in range(1, 11)])\n",
        "submission_df.to_csv(\"submission.csv\", index=False)\n",
        "print(\"Ensembled model trained and predictions saved to submission.csv\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Import libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "\n",
        "# STEP 2: Load the datasets\n",
        "train = pd.read_csv(\"/content/train.csv\")\n",
        "test = pd.read_csv(\"/content/test.csv\")\n",
        "\n",
        "# STEP 3: Separate features and targets\n",
        "# Drop 'ID' from test set and BlendProperty columns from train set\n",
        "X_train = train.drop([f\"BlendProperty{i}\" for i in range(1, 11)], axis=1)\n",
        "y_train = train[[f\"BlendProperty{i}\" for i in range(1, 11)]]\n",
        "X_test = test.drop(\"ID\", axis=1)\n",
        "\n",
        "\n",
        "# STEP 4: Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Optional: convert back to DataFrame\n",
        "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
        "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
        "\n",
        "# STEP 5: Split train data for validation\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(X_train_scaled, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# STEP 6: Train Random Forest Model\n",
        "rf_model = MultiOutputRegressor(RandomForestRegressor(n_estimators=100, random_state=42))\n",
        "rf_model.fit(X_tr, y_tr)\n",
        "\n",
        "# STEP 7: Validate on validation set\n",
        "y_val_pred = rf_model.predict(X_val)\n",
        "mape = mean_absolute_percentage_error(y_val, y_val_pred)\n",
        "print(f\"Validation MAPE: {mape:.4f}\")\n",
        "\n",
        "# STEP 8: Predict on test set\n",
        "y_test_pred = rf_model.predict(X_test_scaled)\n",
        "\n",
        "# STEP 9: Create submission DataFrame\n",
        "submission = pd.DataFrame(y_test_pred, columns=[f\"BlendProperty{i}\" for i in range(1, 11)])\n",
        "submission.insert(0, 'ID', test['ID'])  # Ensure 'ID' is the first column\n",
        "\n",
        "# STEP 10: Save to CSV\n",
        "submission.to_csv(\"random_forest_submission.csv\", index=False)\n",
        "print(\"✅ Submission file saved as 'random_forest_submission.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0cD12ZtkKW0",
        "outputId": "3eb3238a-56dd-4f57-ae52-113f6f203c2a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation MAPE: 4.2471\n",
            "✅ Submission file saved as 'random_forest_submission.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Libraries\n",
        "import pandas as pd, numpy as np, hashlib, optuna, lightgbm as lgb\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 2. Load\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "test  = pd.read_csv(\"test.csv\")\n",
        "\n",
        "# 3. Basic split\n",
        "X_base_train = train.iloc[:, :55].copy()\n",
        "y_train      = train.iloc[:, 55:].copy()\n",
        "X_base_test  = test.iloc[:, :55].copy() # Keep ID for now to merge later\n",
        "\n",
        "\n",
        "# 4. === Feature Engineering =================================================\n",
        "def add_features(df):\n",
        "    out = df.copy()\n",
        "\n",
        "    # component fraction columns = first 5\n",
        "    frac_cols = [f\"Component{i}_fraction\" for i in range(1, 6)]\n",
        "\n",
        "\n",
        "    # --- weighted component-property: (fraction_i * Component_i_Property_j)\n",
        "    for comp in range(1, 6):          # Components 1..5\n",
        "        frac_col = f\"Component{comp}_fraction\"\n",
        "        if frac_col in df.columns: # Ensure the fraction column exists\n",
        "            for prop in range(1, 11):     # Properties 1..10\n",
        "                prop_col = f\"Component{comp}_Property{prop}\"\n",
        "                new_col  = f\"W_{prop_col}\"\n",
        "                # Check if the property column exists in the current DataFrame before using it\n",
        "                if prop_col in df.columns:\n",
        "                    out[new_col] = df[frac_col] * df[prop_col]\n",
        "                else:\n",
        "                    # If property column is missing, create the weighted column with zeros\n",
        "                    out[new_col] = 0\n",
        "\n",
        "\n",
        "    # --- simple pairwise fraction interactions\n",
        "    for i in range(1, 6):\n",
        "        for j in range(i+1, 6):\n",
        "            frac_i_col = f\"Component{i}_fraction\"\n",
        "            frac_j_col = f\"Component{j}_fraction\"\n",
        "            if frac_i_col in df.columns and frac_j_col in df.columns:\n",
        "                 out[f\"Frac_{i}_{j}\"] = df[frac_i_col] * df[frac_j_col]\n",
        "            else:\n",
        "                out[f\"Frac_{i}_{j}\"] = 0\n",
        "\n",
        "\n",
        "    return out\n",
        "\n",
        "# Drop 'ID' from test features before adding features\n",
        "X_train = add_features(X_base_train)\n",
        "X_test  = add_features(X_base_test.drop(\"ID\", axis=1))\n",
        "\n",
        "\n",
        "# 5. Scaling numeric columns (LightGBM doesn't need it, but interactions benefit)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# Ensure X_test has the same columns as X_train before scaling\n",
        "# Add missing columns to X_test and fill with 0\n",
        "missing_cols_in_test = set(X_train.columns) - set(X_test.columns)\n",
        "for c in missing_cols_in_test:\n",
        "    X_test[c] = 0\n",
        "# Reorder columns to match X_train\n",
        "X_test = X_test[X_train.columns]\n",
        "\n",
        "\n",
        "X_test_scaled  = scaler.transform(X_test)\n",
        "\n",
        "# Re‑wrap as DataFrame for convenience\n",
        "X_train = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
        "X_test  = pd.DataFrame(X_test_scaled,  columns=X_test.columns)\n",
        "\n",
        "# 6. GroupKFold for robust CV  (group by rounded fractions hash)\n",
        "def hash_frac(row, precision=2):\n",
        "    # Ensure we only use the fraction columns for hashing\n",
        "    frac_values = row[[f\"Component{i}_fraction\" for i in range(1, 6)]]\n",
        "    key = tuple(np.round(frac_values, precision))\n",
        "    return int(hashlib.md5(str(key).encode()).hexdigest(), 16) % 10_000_000\n",
        "\n",
        "# Apply hash_frac to the original X_base_train to get groups\n",
        "groups = X_base_train.apply(hash_frac, axis=1)\n",
        "\n",
        "\n",
        "gkf = GroupKFold(n_splits=5)\n",
        "\n",
        "# 7. LightGBM + Optuna tuner for ONE target; wrap in a function\n",
        "def tune_and_train(X, y, target_name):\n",
        "    def objective(trial):\n",
        "        params = {\n",
        "            \"objective\": \"rmse\",\n",
        "            \"metric\": \"mae\",\n",
        "            \"verbosity\": -1,\n",
        "            \"boosting_type\": \"gbdt\",\n",
        "            \"learning_rate\": trial.suggest_float(\"lr\", 0.01, 0.2, log=True),\n",
        "            \"num_leaves\": trial.suggest_int(\"leaves\", 31, 1023, log=True),\n",
        "            \"feature_fraction\": trial.suggest_float(\"feat_frac\", 0.5, 1.0),\n",
        "            \"bagging_fraction\": trial.suggest_float(\"bag_frac\", 0.5, 1.0),\n",
        "            \"bagging_freq\": 1,\n",
        "            \"min_data_in_leaf\": trial.suggest_int(\"min_leaf\", 20, 200),\n",
        "            \"lambda_l1\": trial.suggest_float(\"l1\", 0.0, 5.0),\n",
        "            \"lambda_l2\": trial.suggest_float(\"l2\", 0.0, 5.0),\n",
        "        }\n",
        "        mape_scores = []\n",
        "        for train_idx, val_idx in gkf.split(X, y, groups):\n",
        "            lgb_train = lgb.Dataset(X.iloc[train_idx], y.iloc[train_idx])\n",
        "            lgb_val   = lgb.Dataset(X.iloc[val_idx],  y.iloc[val_idx])\n",
        "            # Update lgb.train call to use callbacks for early stopping\n",
        "            model = lgb.train(params, lgb_train,\n",
        "                              valid_sets=[lgb_val],\n",
        "                              num_boost_round=500,\n",
        "                              callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)]) # Use callbacks\n",
        "            preds = model.predict(X.iloc[val_idx])\n",
        "            mape_scores.append(mean_absolute_percentage_error(y.iloc[val_idx], preds))\n",
        "        return np.mean(mape_scores)\n",
        "\n",
        "    study = optuna.create_study(direction=\"minimize\")\n",
        "    study.optimize(objective, n_trials=20, show_progress_bar=False)\n",
        "\n",
        "    best_params = study.best_trial.params\n",
        "    best_params.update({\"objective\": \"rmse\", \"metric\": \"mae\", \"verbosity\": -1})\n",
        "    # Update final lgb.train call to use callbacks for early stopping if needed,\n",
        "    # but since we train on the full data here, early stopping isn't typically used\n",
        "    # and the num_boost_round is taken from the best iteration from Optuna.\n",
        "    # If Optuna's best iteration is not available, default to 500 rounds.\n",
        "    final_model = lgb.train(best_params, lgb.Dataset(X, y),\n",
        "                            num_boost_round=study.best_trial.user_attrs.get(\"best_iteration\", 500))\n",
        "    return final_model, study.best_value\n",
        "\n",
        "# 8. Train one model per target\n",
        "models, val_mapes = {}, {}\n",
        "for target in y_train.columns:\n",
        "    model, best_mape = tune_and_train(X_train, y_train[target], target)\n",
        "    models[target] = model\n",
        "    val_mapes[target] = best_mape\n",
        "    print(f\"{target}: CV‑MAPE {best_mape:.4f}\")\n",
        "\n",
        "print(f\"\\nMean CV‑MAPE over all targets: {np.mean(list(val_mapes.values())):.4f}\")\n",
        "\n",
        "# 9. Predict on test\n",
        "test_preds = pd.DataFrame({t: m.predict(X_test) for t, m in models.items()})\n",
        "\n",
        "# 10. Build submission\n",
        "submission = test_preds.copy()\n",
        "submission.insert(0, \"ID\", test[\"ID\"])\n",
        "submission.to_csv(\"lightgbm_optuna_submission.csv\", index=False)\n",
        "print(\"\\n✅  Submission saved as 'lightgbm_optuna_submission.csv'\")"
      ],
      "metadata": {
        "id": "Qkqnk-BkmaI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13d24963"
      },
      "source": [
        "%pip install optuna"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# blend_prediction.py\n",
        "# Shell.ai Hackathon – Fuel‑Blend Properties Prediction\n",
        "# Author: Safwan (stock‑gpt) – July 2025\n",
        "\n",
        "import os, hashlib, warnings, argparse\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "import lightgbm as lgb\n",
        "from catboost import CatBoostRegressor\n",
        "import optuna, joblib, json, random\n",
        "import sys\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 1. CLI args\n",
        "# ------------------------------------------------------------------------------\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--train\", default=\"train.csv\")\n",
        "parser.add_argument(\"--test\",  default=\"test.csv\")\n",
        "parser.add_argument(\"--out\",   default=\"submission.csv\")\n",
        "parser.add_argument(\"--trials\", type=int, default=60, help=\"Optuna trials per target\")\n",
        "parser.add_argument(\"--lgb_weight\", type=float, default=0.6, help=\"Blend weight for LightGBM\")\n",
        "\n",
        "# Check if running in an interactive environment like Colab and skip argparse if no explicit args\n",
        "if not any(arg.startswith('--') for arg in sys.argv[1:]):\n",
        "    args = argparse.Namespace(train='train.csv', test='test.csv', out='submission.csv', trials=60, lgb_weight=0.6)\n",
        "else:\n",
        "    args = parser.parse_args()\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2. Load\n",
        "# ------------------------------------------------------------------------------\n",
        "train = pd.read_csv(args.train)\n",
        "test  = pd.read_csv(args.test)\n",
        "\n",
        "X_base_train = train.iloc[:, :55].copy()\n",
        "y_train      = train.iloc[:, 55:].copy()\n",
        "X_base_test  = test.iloc[:, :55].copy()\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 3. Feature engineering\n",
        "# ------------------------------------------------------------------------------\n",
        "def add_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    out = df.copy()\n",
        "    # weighted component‑property\n",
        "    for comp in range(1, 6):\n",
        "        f_col = f\"Component{comp}_fraction\"\n",
        "        if f_col not in df: continue\n",
        "        for prop in range(1, 11):\n",
        "            p_col = f\"Component{comp}_Property{prop}\"\n",
        "            new_c = f\"W_Comp{comp}_Prop{prop}\"\n",
        "            out[new_c] = df[f_col] * df.get(p_col, 0)\n",
        "    # pairwise fraction interactions\n",
        "    for i in range(1, 6):\n",
        "        for j in range(i+1, 6):\n",
        "            fi, fj = f\"Component{i}_fraction\", f\"Component{j}_fraction\"\n",
        "            out[f\"Frac_{i}_{j}\"] = df.get(fi, 0) * df.get(fj, 0)\n",
        "    return out\n",
        "\n",
        "X_train = add_features(X_base_train)\n",
        "X_test  = add_features(X_base_test)\n",
        "\n",
        "# ensure same columns order\n",
        "missing = set(X_train.columns) - set(X_test.columns)\n",
        "for c in missing: X_test[c] = 0\n",
        "X_test = X_test[X_train.columns]\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 4. Scaling\n",
        "# ------------------------------------------------------------------------------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled  = scaler.transform(X_test)\n",
        "X_train = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
        "X_test  = pd.DataFrame(X_test_scaled,  columns=X_test.columns)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 5. GroupKFold groups (hash of fractions)\n",
        "# ------------------------------------------------------------------------------\n",
        "def hash_frac(row, prec=2):\n",
        "    key = tuple(np.round(row[[f\"Component{i}_fraction\" for i in range(1,6)]], prec))\n",
        "    return int(hashlib.md5(str(key).encode()).hexdigest(),16)%10_000_000\n",
        "\n",
        "groups = X_base_train.apply(hash_frac, axis=1)\n",
        "gkf = GroupKFold(n_splits=5)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 6. LightGBM + Optuna tuning (bag 5 folds)\n",
        "# ------------------------------------------------------------------------------\n",
        "# Check for GPU availability using CatBoost's method for consistency\n",
        "has_gpu = CatBoostRegressor().get_param(\"task_type\")==\"GPU\" if os.getenv(\"CUDA_VISIBLE_DEVICES\") else False\n",
        "lgb_device = \"gpu\" if has_gpu else \"cpu\"\n",
        "print(f\"🔧 Using LightGBM on {lgb_device.upper()}\")\n",
        "\n",
        "\n",
        "lgb_models = {t: [] for t in y_train.columns}\n",
        "val_mapes  = {}\n",
        "study_db   = optuna.storages.InMemoryStorage()\n",
        "\n",
        "def objective_factory(X, y):\n",
        "    def obj(trial):\n",
        "        params = {\n",
        "            \"objective\": \"rmse\",\n",
        "            \"metric\": \"mae\",\n",
        "            \"verbosity\": -1,\n",
        "            \"boosting_type\": \"gbdt\",\n",
        "            \"device\": lgb_device, # Use the determined device\n",
        "            \"learning_rate\": trial.suggest_float(\"lr\", 0.01, 0.2, log=True),\n",
        "            \"num_leaves\": trial.suggest_int(\"leaves\", 31, 1023, log=True),\n",
        "            \"feature_fraction\": trial.suggest_float(\"feat_frac\", 0.5, 1.0),\n",
        "            \"bagging_fraction\": trial.suggest_float(\"bag_frac\", 0.5, 1.0),\n",
        "            \"bagging_freq\": 1,\n",
        "            \"min_data_in_leaf\": trial.suggest_int(\"min_leaf\", 20, 200),\n",
        "            \"lambda_l1\": trial.suggest_float(\"l1\", 0.0, 5.0),\n",
        "            \"lambda_l2\": trial.suggest_float(\"l2\", 0.0, 5.0),\n",
        "            \"seed\": SEED,\n",
        "        }\n",
        "        mape_scores=[]\n",
        "        for tr, vl in gkf.split(X, y, groups):\n",
        "            m = lgb.train(params, lgb.Dataset(X.iloc[tr], y.iloc[tr]),\n",
        "                          num_boost_round=2000,\n",
        "                          valid_sets=[lgb.Dataset(X.iloc[vl], y.iloc[vl])],\n",
        "                          callbacks=[lgb.early_stopping(100, verbose=False)])\n",
        "            pr = m.predict(X.iloc[vl])\n",
        "            mape_scores.append(mean_absolute_percentage_error(y.iloc[vl], pr))\n",
        "        return np.mean(mape_scores)\n",
        "    return obj\n",
        "\n",
        "for tgt in y_train.columns:\n",
        "    print(f\"\\n🔎 Tuning LightGBM for {tgt} …\")\n",
        "    study = optuna.create_study(direction=\"minimize\", storage=study_db, sampler=optuna.samplers.TPESampler(seed=SEED))\n",
        "    study.optimize(objective_factory(X_train, y_train[tgt]), n_trials=args.trials, show_progress_bar=False)\n",
        "    best = study.best_trial.params\n",
        "    best.update({\"objective\":\"rmse\",\"metric\":\"mae\",\"verbosity\":-1,\"device\":lgb_device,\"seed\":SEED}) # Use the determined device\n",
        "    fold_mapes=[]\n",
        "    for fold,(tr,vl) in enumerate(gkf.split(X_train, y_train[tgt], groups)):\n",
        "        mdl = lgb.train(best, lgb.Dataset(X_train.iloc[tr], y_train[tgt].iloc[tr]),\n",
        "                        num_boost_round=study.best_trial.user_attrs.get(\"best_iteration\", 1000))\n",
        "        lgb_models[tgt].append(mdl)\n",
        "        pr = mdl.predict(X_train.iloc[vl])\n",
        "        fold_mapes.append(mean_absolute_percentage_error(y_train[tgt].iloc[vl], pr))\n",
        "    val_mapes[tgt] = np.mean(fold_mapes)\n",
        "    print(f\"📊 {tgt} CV‑MAPE {val_mapes[tgt]:.4f}\")\n",
        "\n",
        "print(f\"\\n📈 Mean CV‑MAPE: {np.mean(list(val_mapes.values())):.4f}\")\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 7. CatBoost quick model (no tuning, GPU if available)\n",
        "# ------------------------------------------------------------------------------\n",
        "cat_models = {}\n",
        "has_gpu = CatBoostRegressor().get_param(\"task_type\")==\"GPU\" if os.getenv(\"CUDA_VISIBLE_DEVICES\") else False\n",
        "cat_params = dict(\n",
        "    iterations=1200,\n",
        "    depth=8,\n",
        "    learning_rate=0.05,\n",
        "    loss_function=\"MAE\",\n",
        "    task_type=\"GPU\" if has_gpu else \"CPU\",\n",
        "    verbose=False,\n",
        "    random_seed=SEED,\n",
        ")\n",
        "\n",
        "print(f\"\\n🚂 Training CatBoost ({'GPU' if has_gpu else 'CPU'}) …\")\n",
        "for tgt in y_train.columns:\n",
        "    cat = CatBoostRegressor(**cat_params)\n",
        "    cat.fit(X_train, y_train[tgt])\n",
        "    cat_models[tgt] = cat\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 8. Predict & blend\n",
        "# ------------------------------------------------------------------------------\n",
        "preds_lgb = {}\n",
        "for tgt, mdl_list in lgb_models.items():\n",
        "    fold_preds = np.mean([m.predict(X_test) for m in mdl_list], axis=0)\n",
        "    preds_lgb[tgt] = fold_preds\n",
        "\n",
        "preds_cat = {tgt: mdl.predict(X_test) for tgt, mdl in cat_models.items()}\n",
        "\n",
        "alpha = args.lgb_weight\n",
        "blend_preds = {tgt: alpha*preds_lgb[tgt] + (1-alpha)*preds_cat[tgt] for tgt in y_train.columns}\n",
        "\n",
        "submission = pd.DataFrame(blend_preds)\n",
        "submission.insert(0, \"ID\", test[\"ID\"])\n",
        "submission.to_csv(args.out, index=False)\n",
        "\n",
        "print(f\"\\n✅ Submission saved to “{args.out}”\")\n",
        "print(\"    You can now upload it to the leaderboard.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZ8077bwxWyg",
        "outputId": "fbf3c193-7b66-41a1-eb71-083783731702"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-07-07 07:33:35,683] A new study created in memory with name: no-name-370ac956-1970-44ab-ba21-8b2ad71baf96\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔧 Using LightGBM on CPU\n",
            "\n",
            "🔎 Tuning LightGBM for BlendProperty1 …\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-07-07 07:34:04,545] Trial 0 finished with value: 1.9369180722543067 and parameters: {'lr': 0.030710573677773714, 'leaves': 861, 'feat_frac': 0.8659969709057025, 'bag_frac': 0.7993292420985183, 'min_leaf': 48, 'l1': 0.7799726016810132, 'l2': 0.2904180608409973}. Best is trial 0 with value: 1.9369180722543067.\n",
            "[I 2025-07-07 07:34:08,232] Trial 1 finished with value: 8.452601442474162 and parameters: {'lr': 0.13394334706750485, 'leaves': 252, 'feat_frac': 0.8540362888980227, 'bag_frac': 0.5102922471479012, 'min_leaf': 195, 'l1': 4.162213204002109, 'l2': 1.0616955533913808}. Best is trial 0 with value: 1.9369180722543067.\n",
            "[I 2025-07-07 07:34:34,978] Trial 2 finished with value: 1.5734134665240262 and parameters: {'lr': 0.017240892195821537, 'leaves': 58, 'feat_frac': 0.6521211214797689, 'bag_frac': 0.762378215816119, 'min_leaf': 98, 'l1': 1.4561457009902097, 'l2': 3.0592644736118975}. Best is trial 2 with value: 1.5734134665240262.\n",
            "[I 2025-07-07 07:34:53,123] Trial 3 finished with value: 2.3965223259219157 and parameters: {'lr': 0.01518747922672247, 'leaves': 85, 'feat_frac': 0.6831809216468459, 'bag_frac': 0.728034992108518, 'min_leaf': 162, 'l1': 0.9983689107917987, 'l2': 2.571172192068058}. Best is trial 2 with value: 1.5734134665240262.\n",
            "[I 2025-07-07 07:35:04,298] Trial 4 finished with value: 1.719149839805422 and parameters: {'lr': 0.05898602410432694, 'leaves': 36, 'feat_frac': 0.8037724259507192, 'bag_frac': 0.5852620618436457, 'min_leaf': 31, 'l1': 4.7444276862666666, 'l2': 4.828160165372797}. Best is trial 2 with value: 1.5734134665240262.\n",
            "[I 2025-07-07 07:35:10,043] Trial 5 finished with value: 3.9267674564356803 and parameters: {'lr': 0.11265466963346032, 'leaves': 89, 'feat_frac': 0.5488360570031919, 'bag_frac': 0.8421165132560784, 'min_leaf': 99, 'l1': 0.6101911742238941, 'l2': 2.475884550556351}. Best is trial 2 with value: 1.5734134665240262.\n",
            "[I 2025-07-07 07:35:36,714] Trial 6 finished with value: 1.2627491265373918 and parameters: {'lr': 0.011085122517311707, 'leaves': 744, 'feat_frac': 0.6293899908000085, 'bag_frac': 0.831261142176991, 'min_leaf': 76, 'l1': 2.600340105889054, 'l2': 2.7335513967163982}. Best is trial 6 with value: 1.2627491265373918.\n",
            "[I 2025-07-07 07:36:01,926] Trial 7 finished with value: 1.181981144419995 and parameters: {'lr': 0.017398074711291726, 'leaves': 920, 'feat_frac': 0.8875664116805573, 'bag_frac': 0.9697494707820946, 'min_leaf': 181, 'l1': 2.9894998940554256, 'l2': 4.609371175115584}. Best is trial 7 with value: 1.181981144419995.\n",
            "[I 2025-07-07 07:36:21,123] Trial 8 finished with value: 1.5902379580408088 and parameters: {'lr': 0.01303561122512888, 'leaves': 61, 'feat_frac': 0.522613644455269, 'bag_frac': 0.6626651653816322, 'min_leaf': 90, 'l1': 1.3567451588694794, 'l2': 4.143687545759647}. Best is trial 7 with value: 1.181981144419995.\n",
            "[I 2025-07-07 07:36:34,298] Trial 9 finished with value: 1.2765199886797203 and parameters: {'lr': 0.02911701023242742, 'leaves': 82, 'feat_frac': 0.7713480415791243, 'bag_frac': 0.5704621124873813, 'min_leaf': 165, 'l1': 0.3727532183988541, 'l2': 4.9344346830025865}. Best is trial 7 with value: 1.181981144419995.\n",
            "[I 2025-07-07 07:36:51,490] Trial 10 finished with value: 1.8001964587865333 and parameters: {'lr': 0.046589863196059025, 'leaves': 357, 'feat_frac': 0.9847685553939328, 'bag_frac': 0.9775699684882245, 'min_leaf': 137, 'l1': 2.8760403340763627, 'l2': 3.6518344760350483}. Best is trial 7 with value: 1.181981144419995.\n",
            "[I 2025-07-07 07:37:29,282] Trial 11 finished with value: 2.88159433040184 and parameters: {'lr': 0.010193060666576633, 'leaves': 921, 'feat_frac': 0.9801916705804354, 'bag_frac': 0.963652501029298, 'min_leaf': 71, 'l1': 2.9282553604835115, 'l2': 1.645045800722536}. Best is trial 7 with value: 1.181981144419995.\n",
            "[I 2025-07-07 07:37:49,013] Trial 12 finished with value: 1.7418287708779492 and parameters: {'lr': 0.021939308288506758, 'leaves': 483, 'feat_frac': 0.6396470776224414, 'bag_frac': 0.8838129988503955, 'min_leaf': 127, 'l1': 2.3421348647931386, 'l2': 3.8696680737802516}. Best is trial 7 with value: 1.181981144419995.\n",
            "[I 2025-07-07 07:38:11,169] Trial 13 finished with value: 1.384225770007247 and parameters: {'lr': 0.010431891094960023, 'leaves': 491, 'feat_frac': 0.8947385331597323, 'bag_frac': 0.9132177746552786, 'min_leaf': 199, 'l1': 3.5294545526617096, 'l2': 1.7139964413315811}. Best is trial 7 with value: 1.181981144419995.\n",
            "[I 2025-07-07 07:38:33,221] Trial 14 finished with value: 2.7267626340877866 and parameters: {'lr': 0.02197901162815373, 'leaves': 159, 'feat_frac': 0.7190650837455671, 'bag_frac': 0.9035630633125403, 'min_leaf': 64, 'l1': 2.4825583921605685, 'l2': 3.277874290241491}. Best is trial 7 with value: 1.181981144419995.\n",
            "[I 2025-07-07 07:38:40,788] Trial 15 finished with value: 2.6903110899895584 and parameters: {'lr': 0.0779582456544038, 'leaves': 654, 'feat_frac': 0.5895974542510894, 'bag_frac': 0.9968133292225761, 'min_leaf': 131, 'l1': 2.0041389566504204, 'l2': 4.392513802747957}. Best is trial 7 with value: 1.181981144419995.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7871e02",
        "outputId": "89adaf2a-2383-4815-8570-b63c5a79d364"
      },
      "source": [
        "%pip install catboost"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost) (1.15.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (8.5.0)\n",
            "Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO+TjaYaU/OEm2tuLU3HVCQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}